#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆå¤§æ¨¡å‹SFTå¾®è°ƒè®­ç»ƒè„šæœ¬
é€‚åˆRTX 3060ç­‰ä¸­ç«¯æ˜¾å¡
"""

import os
import sys
import json
import torch
import logging
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def check_gpu():
    """æ£€æŸ¥GPUçŠ¶æ€"""
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
        logger.info(f"GPU: {device_name}")
        logger.info(f"æ˜¾å­˜: {total_memory:.1f} GB")
        return True
    else:
        logger.warning("æœªæ£€æµ‹åˆ°CUDA GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰")
        return False

def create_simple_dataset():
    """åˆ›å»ºç®€å•çš„ç¤ºä¾‹æ•°æ®é›†"""
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs("./data/processed", exist_ok=True)
    os.makedirs("./data/raw", exist_ok=True)
    
    # åˆ›å»ºå°è§„æ¨¡é«˜è´¨é‡è®­ç»ƒæ•°æ®
    train_data = [
        {
            "instruction": "What is machine learning?",
            "input": "",
            "output": "Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed for every task."
        },
        {
            "instruction": "Explain what is Python",
            "input": "",
            "output": "Python is a high-level, interpreted programming language known for its simple syntax and readability. It's widely used for web development, data science, automation, and artificial intelligence."
        },
        {
            "instruction": "How to sort a list in Python?",
            "input": "",
            "output": "You can sort a list in Python using the sort() method for in-place sorting: my_list.sort(), or using the sorted() function to return a new sorted list: sorted_list = sorted(my_list)."
        },
        {
            "instruction": "What is the difference between AI and ML?",
            "input": "",
            "output": "AI (Artificial Intelligence) is the broader concept of machines being able to carry out tasks in a smart way. ML (Machine Learning) is a subset of AI that focuses on machines learning from data to make predictions or decisions."
        },
        {
            "instruction": "Translate to Chinese",
            "input": "Hello world",
            "output": "ä½ å¥½ï¼Œä¸–ç•Œ"
        },
        {
            "instruction": "Write a simple function",
            "input": "Calculate factorial",
            "output": "def factorial(n):\n    if n <= 1:\n        return 1\n    return n * factorial(n - 1)"
        },
        {
            "instruction": "Explain deep learning",
            "input": "",
            "output": "Deep learning is a subset of machine learning that uses artificial neural networks with multiple layers to model and understand complex patterns in data. It's particularly effective for tasks like image recognition and natural language processing."
        },
        {
            "instruction": "What is LoRA?",
            "input": "",
            "output": "LoRA (Low-Rank Adaptation) is a technique for efficiently fine-tuning large language models by adding small trainable matrices to existing model weights, significantly reducing the number of trainable parameters."
        }
    ]
    
    # éªŒè¯æ•°æ®ï¼ˆç®€å•åˆ†å‰²ï¼‰
    val_data = [
        {
            "instruction": "What is GPU?",
            "input": "",
            "output": "GPU (Graphics Processing Unit) is a specialized processor designed to handle graphics rendering and parallel computations, making it ideal for machine learning and AI tasks."
        },
        {
            "instruction": "How to install Python packages?",
            "input": "",
            "output": "You can install Python packages using pip: 'pip install package_name' in the command line, or using conda: 'conda install package_name' if you're using Anaconda."
        }
    ]
    
    # ä¿å­˜æ•°æ®é›†
    with open("./data/processed/train.json", 'w', encoding='utf-8') as f:
        json.dump(train_data, f, ensure_ascii=False, indent=2)
    
    with open("./data/processed/val.json", 'w', encoding='utf-8') as f:
        json.dump(val_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"è®­ç»ƒæ•°æ®: {len(train_data)} æ¡")
    logger.info(f"éªŒè¯æ•°æ®: {len(val_data)} æ¡")
    
    return train_data, val_data

def simple_training_demo():
    """ç®€å•çš„è®­ç»ƒæ¼”ç¤ºï¼ˆä¸éœ€è¦å¤æ‚ä¾èµ–ï¼‰"""
    logger.info("=" * 60)
    logger.info("å¤§æ¨¡å‹SFTå¾®è°ƒé¡¹ç›® - RTX 3060ç‰ˆæœ¬")
    logger.info("=" * 60)
    
    # æ£€æŸ¥GPU
    has_gpu = check_gpu()
    
    # åˆ›å»ºæ•°æ®é›†
    logger.info("åˆ›å»ºç¤ºä¾‹æ•°æ®é›†...")
    train_data, val_data = create_simple_dataset()
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    logger.info("\næ¨èé…ç½®ï¼ˆé€‚åˆRTX 3060 12GBï¼‰:")
    logger.info("- æ¨¡å‹: microsoft/DialoGPT-medium (350Må‚æ•°)")
    logger.info("- å¤‡é€‰: distilgpt2 (82M), gpt2 (124M), TinyLlama-1.1B")
    logger.info("- é‡åŒ–: 4bité‡åŒ–")
    logger.info("- LoRA rank: 16")
    logger.info("- æ‰¹å¤§å°: 4")
    logger.info("- åºåˆ—é•¿åº¦: 512")
    logger.info("- ç²¾åº¦: FP16")
    logger.info("- ğŸ¯ æç¤ºè¯å·¥ç¨‹: è§’è‰²æ‰®æ¼”ã€æ€ç»´é“¾ã€Few-Shot")
    logger.info("- ğŸ” RAGæŠ€æœ¯: è½»é‡çº§æ£€ç´¢å¢å¼ºç”Ÿæˆ")
    
    # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨å»ºè®®
    if has_gpu:
        allocated = torch.cuda.memory_allocated() / (1024**3)
        reserved = torch.cuda.memory_reserved() / (1024**3)
        logger.info(f"\nå½“å‰GPUå†…å­˜ä½¿ç”¨:")
        logger.info(f"- å·²åˆ†é…: {allocated:.2f} GB")
        logger.info(f"- å·²ä¿ç•™: {reserved:.2f} GB")
    
    logger.info("\nè¦å¼€å§‹è®­ç»ƒï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤:")
    logger.info("1. å®‰è£…ä¾èµ–: pip install -r requirements.txt")
    logger.info("2. ç”Ÿæˆæç¤ºè¯å·¥ç¨‹æ•°æ®: python prompt_engineering_data.py")
    logger.info("3. åˆå§‹åŒ–RAGçŸ¥è¯†åº“: python rag_system.py")
    logger.info("4. è¿è¡Œè®­ç»ƒ: python train.py --config configs/training_config.yaml")
    logger.info("5. æµ‹è¯•RAGç³»ç»Ÿ: python test_rag.py")
    logger.info("6. æ¨ç†æµ‹è¯•: python inference.py --model_path ./output/final_model")
    
    logger.info("\næ³¨æ„äº‹é¡¹:")
    logger.info("- ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´ï¼ˆè‡³å°‘5GBï¼‰")
    logger.info("- è®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§GPUæ¸©åº¦")
    logger.info("- å¯ä»¥æ ¹æ®æ˜¾å­˜ä½¿ç”¨æƒ…å†µè°ƒæ•´batch_size")
    logger.info("- ğŸ¯ æç¤ºè¯å·¥ç¨‹å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹æ•ˆæœ")
    logger.info("- ğŸ” RAGæŠ€æœ¯è®©å°æ¨¡å‹å…·å¤‡å¤§æ¨¡å‹çš„çŸ¥è¯†å¹¿åº¦")
    
    return True

def create_requirements_txt():
    """åˆ›å»ºé€‚åˆRTX 3060çš„requirements.txt"""
    requirements = """# æ ¸å¿ƒä¾èµ–ï¼ˆRTX 3060ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
torch>=2.0.0
transformers>=4.35.0
peft>=0.6.0
datasets>=2.14.0
accelerate>=0.24.0
bitsandbytes>=0.41.0

# åŸºç¡€ä¾èµ–
numpy>=1.24.0
tqdm>=4.65.0
pyyaml>=6.0

# å¯é€‰ä¾èµ–ï¼ˆç”¨äºè¯„ä¼°ï¼‰
# sacrebleu>=2.3.0
# rouge-score>=0.1.2
# nltk>=3.8

# å¦‚æœéœ€è¦wandbæ—¥å¿—
# wandb>=0.15.0
"""
    
    with open("requirements_rtx3060.txt", 'w') as f:
        f.write(requirements)
    
    logger.info("å·²åˆ›å»º requirements_rtx3060.txt")

if __name__ == "__main__":
    # åˆ›å»ºé€‚åˆRTX 3060çš„ä¾èµ–æ–‡ä»¶
    create_requirements_txt()
    
    # è¿è¡Œç®€å•æ¼”ç¤º
    simple_training_demo()
    
    print("\n" + "="*60)
    print("é¡¹ç›®åˆ›å»ºå®Œæˆï¼")
    print("="*60)
    print("\nä¸‹ä¸€æ­¥:")
    print("1. pip install -r requirements_rtx3060.txt")
    print("2. ç”Ÿæˆå¢å¼ºæ•°æ®: python prompt_engineering_data.py")
    print("3. åˆå§‹åŒ–RAGç³»ç»Ÿ: python rag_system.py")
    print("4. è¿è¡Œæ•°æ®å‡†å¤‡: python data/prepare_dataset.py")
    print("5. å¼€å§‹è®­ç»ƒ: python train.py --config configs/training_config.yaml")
    print("6. æµ‹è¯•RAGæ•ˆæœ: python test_rag.py")
    print("\nğŸ¯ æ–°åŠŸèƒ½:")
    print("- æç¤ºè¯å·¥ç¨‹: å¤šç§æç¤ºæŠ€æœ¯å¢å¼ºè®­ç»ƒæ•ˆæœ")
    print("- RAGæŠ€æœ¯: æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œæå‡å›ç­”å‡†ç¡®æ€§")
    print("\næ³¨æ„: å¦‚æœé‡åˆ°æ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥:")
    print("- å‡å°‘batch_sizeåˆ°2æˆ–1")
    print("- ä½¿ç”¨æ›´å°çš„æ¨¡å‹å¦‚distilgpt2")
    print("- å‡å°‘max_seq_lengthåˆ°256")
