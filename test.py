#!/usr/bin/env python3
"""
æ¨¡å‹æµ‹è¯•è„šæœ¬ - æµ‹è¯•è®­ç»ƒå®Œæˆçš„LoRAæ¨¡å‹
æ”¯æŒäº¤äº’å¼å¯¹è¯å’Œæ‰¹é‡æµ‹è¯•
"""

import os
import sys
import torch
import json
import logging
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelTester:
    """æ¨¡å‹æµ‹è¯•ç±»"""
    
    def __init__(self, model_path="./output/final_model", base_model="distilgpt2"):
        """
        åˆå§‹åŒ–æµ‹è¯•å™¨
        
        Args:
            model_path: è®­ç»ƒå¥½çš„LoRAæ¨¡å‹è·¯å¾„
            base_model: åŸºç¡€æ¨¡å‹åç§°
        """
        self.model_path = model_path
        self.base_model = base_model
        self.model = None
        self.tokenizer = None
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        logger.info("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        
        try:
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not Path(self.model_path).exists():
                raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
            
            # åŠ è½½tokenizer
            logger.info(f"åŠ è½½tokenizer: {self.base_model}")
            self.tokenizer = AutoTokenizer.from_pretrained(self.base_model)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            logger.info(f"åŠ è½½åŸºç¡€æ¨¡å‹: {self.base_model}")
            base_model = AutoModelForCausalLM.from_pretrained(
                self.base_model,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            
            # åŠ è½½LoRAé€‚é…å™¨
            logger.info(f"åŠ è½½LoRAé€‚é…å™¨: {self.model_path}")
            self.model = PeftModel.from_pretrained(base_model, self.model_path)
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            
            logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            
            # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
            self._show_model_info()
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _show_model_info(self):
        """æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        logger.info("=" * 50)
        logger.info("æ¨¡å‹ä¿¡æ¯:")
        logger.info(f"  åŸºç¡€æ¨¡å‹: {self.base_model}")
        logger.info(f"  LoRAè·¯å¾„: {self.model_path}")
        logger.info(f"  æ€»å‚æ•°é‡: {total_params:,}")
        logger.info(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        logger.info(f"  è®¾å¤‡: {next(self.model.parameters()).device}")
        logger.info("=" * 50)
    
    def generate_response(self, prompt, max_new_tokens=150, temperature=0.7, top_p=0.9, do_sample=True):
        """
        ç”Ÿæˆå›ç­”
        
        Args:
            prompt: è¾“å…¥æç¤º
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: nucleus samplingå‚æ•°
            do_sample: æ˜¯å¦é‡‡æ ·
            
        Returns:
            ç”Ÿæˆçš„å›ç­”
        """
        try:
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=do_sample,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            # è§£ç è¾“å‡º
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # åªè¿”å›æ–°ç”Ÿæˆçš„éƒ¨åˆ†
            new_response = response[len(prompt):].strip()
            
            return new_response
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}")
            return f"é”™è¯¯: {e}"
    
    def test_single_prompt(self, prompt):
        """æµ‹è¯•å•ä¸ªæç¤º"""
        logger.info(f"ğŸ“ è¾“å…¥: {prompt}")
        response = self.generate_response(prompt)
        logger.info(f"ğŸ¤– è¾“å‡º: {response}")
        print(f"\n{'='*60}")
        print(f"è¾“å…¥: {prompt}")
        print(f"è¾“å‡º: {response}")
        print(f"{'='*60}\n")
        return response
    
    def run_batch_tests(self):
        """è¿è¡Œæ‰¹é‡æµ‹è¯•"""
        logger.info("ğŸ§ª å¼€å§‹æ‰¹é‡æµ‹è¯•...")
        
        test_prompts = [
            "Human: What is machine learning?\nAssistant:",
            "Human: è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½\nAssistant:",
            "Human: How does a neural network work?\nAssistant:",
            "Human: ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ\nAssistant:",
            "Human: Explain the concept of supervised learning\nAssistant:",
            "Human: è¯·è§£é‡Šä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†\nAssistant:",
            "Human: What are the benefits of using AI?\nAssistant:",
            "Human: æœºå™¨å­¦ä¹ æœ‰å“ªäº›åº”ç”¨ï¼Ÿ\nAssistant:"
        ]
        
        results = []
        for i, prompt in enumerate(test_prompts, 1):
            logger.info(f"æµ‹è¯• {i}/{len(test_prompts)}")
            response = self.test_single_prompt(prompt)
            results.append({"prompt": prompt, "response": response})
        
        return results
    
    def interactive_chat(self):
        """äº¤äº’å¼èŠå¤©"""
        logger.info("ğŸ¯ è¿›å…¥äº¤äº’å¼èŠå¤©æ¨¡å¼")
        print("\n" + "="*60)
        print("ğŸ¤– AIåŠ©æ‰‹å·²å°±ç»ªï¼è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("="*60)
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ æ‚¨: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                
                if not user_input:
                    continue
                
                # æ ¼å¼åŒ–è¾“å…¥
                prompt = f"Human: {user_input}\nAssistant:"
                
                # ç”Ÿæˆå›ç­”
                response = self.generate_response(prompt, max_new_tokens=150)
                print(f"ğŸ¤– AI: {response}")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ å‡ºé”™äº†: {e}")
    
    def save_test_results(self, results, filename="test_results.json"):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ LoRAæ¨¡å‹æµ‹è¯•å™¨å¯åŠ¨")
    
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        logger.info(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
        logger.info(f"ğŸ’¾ æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        logger.info("âš ï¸  ä½¿ç”¨CPUæ¨¡å¼")
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = ModelTester()
    
    try:
        # åŠ è½½æ¨¡å‹
        tester.load_model()
        
        # è¯¢é—®æµ‹è¯•æ¨¡å¼
        print("\nè¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼:")
        print("1. æ‰¹é‡æµ‹è¯• (è¿è¡Œé¢„è®¾çš„æµ‹è¯•ç”¨ä¾‹)")
        print("2. äº¤äº’å¼èŠå¤© (ä¸AIè¿›è¡Œå¯¹è¯)")
        print("3. å•æ¬¡æµ‹è¯• (è¾“å…¥ä¸€ä¸ªé—®é¢˜)")
        
        while True:
            choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1/2/3): ").strip()
            
            if choice == "1":
                # æ‰¹é‡æµ‹è¯•
                results = tester.run_batch_tests()
                tester.save_test_results(results)
                break
                
            elif choice == "2":
                # äº¤äº’å¼èŠå¤©
                tester.interactive_chat()
                break
                
            elif choice == "3":
                # å•æ¬¡æµ‹è¯•
                prompt = input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
                if prompt:
                    formatted_prompt = f"Human: {prompt}:\nAssistant:"
                    tester.test_single_prompt(formatted_prompt)
                break
                
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1ã€2 æˆ– 3")
    
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
