#!/usr/bin/env python3
"""
RAG(æ£€ç´¢å¢å¼ºç”Ÿæˆ)æŠ€æœ¯æ¨¡å—
é€‚ç”¨äºRTX 3060çš„è½»é‡çº§å®ç°
"""

import os
import json
import torch
import numpy as np
from typing import List, Dict, Any, Tuple
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import logging

# ç®€å•çš„å‘é‡åŒ–å’Œæ£€ç´¢å®ç°ï¼ˆæ— éœ€é¢å¤–GPUæ˜¾å­˜ï¼‰
class SimpleVectorStore:
    """ç®€å•çš„å‘é‡å­˜å‚¨å’Œæ£€ç´¢ç³»ç»Ÿ"""
    
    def __init__(self):
        self.documents = []
        self.embeddings = []
        self.doc_ids = []
    
    def simple_embedding(self, text: str) -> List[float]:
        """ç®€å•çš„æ–‡æœ¬åµŒå…¥ï¼ˆåŸºäºTF-IDFæ€æƒ³ï¼‰"""
        # ç®€å•çš„è¯é¢‘ç»Ÿè®¡
        words = text.lower().split()
        word_count = {}
        for word in words:
            word_count[word] = word_count.get(word, 0) + 1
        
        # åˆ›å»ºç®€å•çš„ç‰¹å¾å‘é‡
        common_words = [
            'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'ç¥ç»ç½‘ç»œ', 'python', 'äººå·¥æ™ºèƒ½', 'ai', 'ml', 
            'ç®—æ³•', 'æ•°æ®', 'æ¨¡å‹', 'è®­ç»ƒ', 'å­¦ä¹ ', 'è®¡ç®—æœº', 'æŠ€æœ¯', 'ç¼–ç¨‹',
            'machine', 'learning', 'deep', 'neural', 'network', 'artificial',
            'intelligence', 'algorithm', 'data', 'model', 'training', 'computer'
        ]
        
        embedding = []
        for word in common_words:
            embedding.append(text.lower().count(word))
        
        # å½’ä¸€åŒ–
        total = sum(embedding) + 1e-8
        embedding = [x / total for x in embedding]
        
        return embedding
    
    def add_document(self, doc_id: str, content: str, metadata: Dict = None):
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨"""
        embedding = self.simple_embedding(content)
        self.documents.append({
            'id': doc_id,
            'content': content,
            'metadata': metadata or {}
        })
        self.embeddings.append(embedding)
        self.doc_ids.append(doc_id)
    
    def similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = sum(a * a for a in vec1) ** 0.5
        norm2 = sum(b * b for b in vec2) ** 0.5
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def search(self, query: str, top_k: int = 3) -> List[Dict]:
        """æœç´¢æœ€ç›¸å…³çš„æ–‡æ¡£"""
        query_embedding = self.simple_embedding(query)
        
        similarities = []
        for i, doc_embedding in enumerate(self.embeddings):
            sim = self.similarity(query_embedding, doc_embedding)
            similarities.append((sim, i))
        
        # æ’åºå¹¶è¿”å›top_k
        similarities.sort(reverse=True)
        
        results = []
        for sim_score, idx in similarities[:top_k]:
            if sim_score > 0.1:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                doc = self.documents[idx].copy()
                doc['similarity'] = sim_score
                results.append(doc)
        
        return results

class KnowledgeBase:
    """çŸ¥è¯†åº“ç®¡ç†å™¨"""
    
    def __init__(self):
        self.vector_store = SimpleVectorStore()
        self.knowledge_data = self._create_knowledge_base()
    
    def _create_knowledge_base(self) -> List[Dict]:
        """åˆ›å»ºç¤ºä¾‹çŸ¥è¯†åº“"""
        knowledge_base = [
            {
                "id": "ml_basics",
                "title": "æœºå™¨å­¦ä¹ åŸºç¡€",
                "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚æœºå™¨å­¦ä¹ ä¸»è¦åˆ†ä¸ºä¸‰ç±»ï¼šç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚ç›‘ç£å­¦ä¹ ä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œå¦‚åˆ†ç±»å’Œå›å½’ä»»åŠ¡ã€‚æ— ç›‘ç£å­¦ä¹ ä»æœªæ ‡è®°æ•°æ®ä¸­å‘ç°éšè—æ¨¡å¼ï¼Œå¦‚èšç±»å’Œé™ç»´ã€‚å¼ºåŒ–å­¦ä¹ é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚",
                "category": "machine_learning",
                "keywords": ["æœºå™¨å­¦ä¹ ", "ç›‘ç£å­¦ä¹ ", "æ— ç›‘ç£å­¦ä¹ ", "å¼ºåŒ–å­¦ä¹ "]
            },
            {
                "id": "deep_learning",
                "title": "æ·±åº¦å­¦ä¹ è¯¦è§£",
                "content": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚æ·±åº¦ç¥ç»ç½‘ç»œåŒ…å«è¾“å…¥å±‚ã€å¤šä¸ªéšè—å±‚å’Œè¾“å‡ºå±‚ã€‚æ¯å±‚åŒ…å«å¤šä¸ªç¥ç»å…ƒï¼Œé€šè¿‡æƒé‡è¿æ¥ã€‚æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚å¸¸è§çš„æ·±åº¦å­¦ä¹ æ¶æ„åŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œ(CNN)ã€å¾ªç¯ç¥ç»ç½‘ç»œ(RNN)å’ŒTransformerã€‚",
                "category": "deep_learning",
                "keywords": ["æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "CNN", "RNN", "Transformer"]
            },
            {
                "id": "python_programming",
                "title": "Pythonç¼–ç¨‹è¯­è¨€",
                "content": "Pythonæ˜¯ä¸€ç§é«˜çº§ã€è§£é‡Šæ€§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½è€Œé—»åã€‚Pythonåœ¨æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ é¢†åŸŸç‰¹åˆ«å—æ¬¢è¿ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„åº“ç”Ÿæ€ç³»ç»Ÿã€‚é‡è¦çš„Pythonåº“åŒ…æ‹¬ï¼šNumPyç”¨äºæ•°å€¼è®¡ç®—ï¼ŒPandasç”¨äºæ•°æ®å¤„ç†ï¼ŒMatplotlibç”¨äºæ•°æ®å¯è§†åŒ–ï¼ŒScikit-learnç”¨äºæœºå™¨å­¦ä¹ ï¼ŒTensorFlowå’ŒPyTorchç”¨äºæ·±åº¦å­¦ä¹ ã€‚Pythonçš„ç®€æ´è¯­æ³•ä½¿å¾—å¿«é€ŸåŸå‹å¼€å‘å’Œç®—æ³•å®ç°å˜å¾—å®¹æ˜“ã€‚",
                "category": "programming",
                "keywords": ["Python", "NumPy", "Pandas", "Scikit-learn", "TensorFlow", "PyTorch"]
            },
            {
                "id": "neural_networks",
                "title": "ç¥ç»ç½‘ç»œå·¥ä½œåŸç†",
                "content": "ç¥ç»ç½‘ç»œæ˜¯ç”±ç›¸äº’è¿æ¥çš„èŠ‚ç‚¹ï¼ˆç¥ç»å…ƒï¼‰ç»„æˆçš„è®¡ç®—æ¨¡å‹ï¼Œæ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»ç³»ç»Ÿçš„å·¥ä½œæ–¹å¼ã€‚æ¯ä¸ªç¥ç»å…ƒæ¥æ”¶å¤šä¸ªè¾“å…¥ï¼Œå¯¹è¾“å…¥è¿›è¡ŒåŠ æƒæ±‚å’Œï¼ŒåŠ ä¸Šåç½®é¡¹ï¼Œç„¶åé€šè¿‡æ¿€æ´»å‡½æ•°äº§ç”Ÿè¾“å‡ºã€‚å¸¸è§çš„æ¿€æ´»å‡½æ•°åŒ…æ‹¬sigmoidã€tanhã€ReLUç­‰ã€‚ç¥ç»ç½‘ç»œé€šè¿‡åå‘ä¼ æ’­ç®—æ³•è¿›è¡Œè®­ç»ƒï¼Œè¯¥ç®—æ³•è®¡ç®—æŸå¤±å‡½æ•°å¯¹æ¯ä¸ªæƒé‡çš„æ¢¯åº¦ï¼Œç„¶åä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°æƒé‡ã€‚",
                "category": "neural_networks",
                "keywords": ["ç¥ç»ç½‘ç»œ", "æ¿€æ´»å‡½æ•°", "åå‘ä¼ æ’­", "æ¢¯åº¦ä¸‹é™"]
            },
            {
                "id": "nlp_basics",
                "title": "è‡ªç„¶è¯­è¨€å¤„ç†åŸºç¡€",
                "content": "è‡ªç„¶è¯­è¨€å¤„ç†(NLP)æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸“æ³¨äºè®¡ç®—æœºä¸äººç±»è¯­è¨€çš„äº¤äº’ã€‚NLPçš„ä¸»è¦ä»»åŠ¡åŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€å‘½åå®ä½“è¯†åˆ«ã€æœºå™¨ç¿»è¯‘ã€é—®ç­”ç³»ç»Ÿã€æ–‡æœ¬æ‘˜è¦ç­‰ã€‚ä¼ ç»Ÿçš„NLPæ–¹æ³•ä¾èµ–äºæ‰‹å·¥ç‰¹å¾å·¥ç¨‹ï¼Œè€Œç°ä»£NLPä¸»è¦åŸºäºæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚Transformeræ¶æ„é©å‘½æ€§åœ°æ”¹è¿›äº†NLPä»»åŠ¡çš„æ€§èƒ½ï¼Œå‚¬ç”Ÿäº†BERTã€GPTã€T5ç­‰å¼ºå¤§çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚",
                "category": "nlp",
                "keywords": ["è‡ªç„¶è¯­è¨€å¤„ç†", "NLP", "æ–‡æœ¬åˆ†ç±»", "BERT", "GPT", "Transformer"]
            },
            {
                "id": "data_science",
                "title": "æ•°æ®ç§‘å­¦æµç¨‹",
                "content": "æ•°æ®ç§‘å­¦æ˜¯ä¸€ä¸ªè·¨å­¦ç§‘é¢†åŸŸï¼Œç»“åˆäº†ç»Ÿè®¡å­¦ã€è®¡ç®—æœºç§‘å­¦å’Œé¢†åŸŸä¸“ä¸šçŸ¥è¯†æ¥ä»æ•°æ®ä¸­æå–æœ‰ä»·å€¼çš„è§è§£ã€‚å…¸å‹çš„æ•°æ®ç§‘å­¦æµç¨‹åŒ…æ‹¬ï¼šæ•°æ®æ”¶é›†ã€æ•°æ®æ¸…æ´—ã€æ¢ç´¢æ€§æ•°æ®åˆ†æã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹é€‰æ‹©å’Œè®­ç»ƒã€æ¨¡å‹è¯„ä¼°å’Œéƒ¨ç½²ã€‚æ•°æ®ç§‘å­¦å®¶éœ€è¦æŒæ¡ç¼–ç¨‹æŠ€èƒ½ï¼ˆPython/Rï¼‰ã€ç»Ÿè®¡çŸ¥è¯†ã€æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œä»¥åŠä¸šåŠ¡ç†è§£èƒ½åŠ›ã€‚",
                "category": "data_science",
                "keywords": ["æ•°æ®ç§‘å­¦", "æ•°æ®æ¸…æ´—", "ç‰¹å¾å·¥ç¨‹", "æ¨¡å‹è¯„ä¼°"]
            },
            {
                "id": "ai_applications",
                "title": "äººå·¥æ™ºèƒ½åº”ç”¨é¢†åŸŸ",
                "content": "äººå·¥æ™ºèƒ½åœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚åœ¨åŒ»ç–—é¢†åŸŸï¼ŒAIç”¨äºåŒ»å­¦å½±åƒåˆ†æã€è¯ç‰©å‘ç°å’Œä¸ªæ€§åŒ–æ²»ç–—ã€‚åœ¨é‡‘èé¢†åŸŸï¼ŒAIç”¨äºé£é™©è¯„ä¼°ã€ç®—æ³•äº¤æ˜“å’Œæ¬ºè¯ˆæ£€æµ‹ã€‚åœ¨äº¤é€šé¢†åŸŸï¼ŒAIæ¨åŠ¨äº†è‡ªåŠ¨é©¾é©¶æ±½è½¦çš„å‘å±•ã€‚åœ¨æ•™è‚²é¢†åŸŸï¼ŒAIå®ç°äº†ä¸ªæ€§åŒ–å­¦ä¹ å’Œæ™ºèƒ½è¾…å¯¼ã€‚åœ¨å¨±ä¹é¢†åŸŸï¼ŒAIç”¨äºæ¨èç³»ç»Ÿå’Œå†…å®¹ç”Ÿæˆã€‚è¿™äº›åº”ç”¨å±•ç¤ºäº†AIæŠ€æœ¯çš„å·¨å¤§æ½œåŠ›å’Œç¤¾ä¼šå½±å“ã€‚",
                "category": "applications",
                "keywords": ["äººå·¥æ™ºèƒ½åº”ç”¨", "åŒ»ç–—AI", "é‡‘èAI", "è‡ªåŠ¨é©¾é©¶", "æ¨èç³»ç»Ÿ"]
            }
        ]
        
        return knowledge_base
    
    def build_index(self):
        """æ„å»ºçŸ¥è¯†åº“ç´¢å¼•"""
        print("ğŸ“š æ„å»ºçŸ¥è¯†åº“ç´¢å¼•...")
        
        for item in self.knowledge_data:
            self.vector_store.add_document(
                doc_id=item["id"],
                content=f"{item['title']} {item['content']}",
                metadata={
                    "title": item["title"],
                    "category": item["category"],
                    "keywords": item["keywords"]
                }
            )
        
        print(f"âœ… çŸ¥è¯†åº“ç´¢å¼•æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(self.knowledge_data)} ä¸ªæ–‡æ¡£")
    
    def search(self, query: str, top_k: int = 3) -> List[Dict]:
        """æœç´¢ç›¸å…³çŸ¥è¯†"""
        return self.vector_store.search(query, top_k)
    
    def save_knowledge_base(self, filepath: str = "./data/knowledge_base.json"):
        """ä¿å­˜çŸ¥è¯†åº“åˆ°æ–‡ä»¶"""
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.knowledge_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ çŸ¥è¯†åº“å·²ä¿å­˜åˆ°: {filepath}")

class RAGSystem:
    """RAGç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self, model_path="./output/final_model", base_model="distilgpt2"):
        self.model_path = model_path
        self.base_model = base_model
        self.model = None
        self.tokenizer = None
        self.knowledge_base = KnowledgeBase()
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        print("ğŸ“¦ åŠ è½½RAGæ¨¡å‹...")
        
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        if os.path.exists(self.model_path):
            base_model = AutoModelForCausalLM.from_pretrained(
                self.base_model,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            self.model = PeftModel.from_pretrained(base_model, self.model_path)
        else:
            print("âš ï¸ å¾®è°ƒæ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.base_model,
                torch_dtype=torch.float16,
                device_map="auto"
            )
        
        self.model.eval()
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def initialize_knowledge_base(self):
        """åˆå§‹åŒ–çŸ¥è¯†åº“"""
        self.knowledge_base.build_index()
        self.knowledge_base.save_knowledge_base()
    
    def generate_rag_response(self, question: str, max_new_tokens: int = 300) -> Dict[str, Any]:
        """ä½¿ç”¨RAGç”Ÿæˆå›ç­”"""
        print(f"ğŸ” RAGå¤„ç†é—®é¢˜: {question}")
        
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        relevant_docs = self.knowledge_base.search(question, top_k=3)
        
        if relevant_docs:
            print(f"ğŸ“‹ æ£€ç´¢åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£:")
            for i, doc in enumerate(relevant_docs):
                print(f"   {i+1}. {doc['metadata']['title']} (ç›¸ä¼¼åº¦: {doc['similarity']:.3f})")
        
        # 2. æ„å»ºå¢å¼ºæç¤º
        if relevant_docs:
            context_parts = []
            for i, doc in enumerate(relevant_docs):
                context_parts.append(f"å‚è€ƒèµ„æ–™{i+1}: {doc['content'][:200]}...")
            
            context = "\n\n".join(context_parts)
            
            enhanced_prompt = f"""è¯·åŸºäºä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ï¼š

{context}

é—®é¢˜: {question}

è¯·ç»“åˆå‚è€ƒèµ„æ–™æä¾›å‡†ç¡®ã€è¯¦ç»†çš„å›ç­”:"""
        else:
            enhanced_prompt = f"é—®é¢˜: {question}\n\nè¯·æä¾›è¯¦ç»†çš„å›ç­”:"
        
        # 3. ç”Ÿæˆå›ç­”
        inputs = self.tokenizer(enhanced_prompt, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.2
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = response[len(enhanced_prompt):].strip()
        
        return {
            "question": question,
            "retrieved_docs": relevant_docs,
            "context_used": len(relevant_docs) > 0,
            "enhanced_prompt": enhanced_prompt,
            "answer": answer
        }

def create_rag_test_script():
    """åˆ›å»ºRAGæµ‹è¯•è„šæœ¬"""
    rag_test_code = '''#!/usr/bin/env python3
"""
RAGç³»ç»Ÿæµ‹è¯•è„šæœ¬
"""

from rag_system import RAGSystem
import json
from datetime import datetime

def main():
    print("ğŸš€ RAGç³»ç»Ÿæµ‹è¯•å¯åŠ¨")
    print("="*60)
    
    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag = RAGSystem()
    
    try:
        # åŠ è½½æ¨¡å‹å’ŒçŸ¥è¯†åº“
        rag.load_model()
        rag.initialize_knowledge_base()
        
        # æµ‹è¯•é—®é¢˜
        test_questions = [
            "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ", 
            "Pythonä¸ºä»€ä¹ˆé€‚åˆåšæ•°æ®ç§‘å­¦ï¼Ÿ",
            "ç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ",
            "è‡ªç„¶è¯­è¨€å¤„ç†æœ‰å“ªäº›åº”ç”¨ï¼Ÿ",
            "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸæœ‰ä»€ä¹ˆåº”ç”¨ï¼Ÿ"
        ]
        
        results = []
        
        print("\\nğŸ§ª å¼€å§‹RAGæµ‹è¯•...")
        print("="*60)
        
        for i, question in enumerate(test_questions, 1):
            print(f"\\nğŸ“ æµ‹è¯• {i}/{len(test_questions)}")
            print("-" * 40)
            
            result = rag.generate_rag_response(question)
            results.append(result)
            
            print(f"é—®é¢˜: {question}")
            print(f"å›ç­”: {result['answer']}")
            print(f"ä½¿ç”¨ä¸Šä¸‹æ–‡: {'æ˜¯' if result['context_used'] else 'å¦'}")
            
        # ä¿å­˜ç»“æœ
        output_file = f"rag_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print("\\n" + "="*60)
        print("âœ… RAGæµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“Š æµ‹è¯•é—®é¢˜æ•°: {len(test_questions)}")
        print(f"ğŸ’¾ ç»“æœä¿å­˜è‡³: {output_file}")
        print("="*60)
        
    except Exception as e:
        print(f"âŒ RAGæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
'''
    
    with open("test_rag.py", 'w', encoding='utf-8') as f:
        f.write(rag_test_code)
    
    print("âœ… RAGæµ‹è¯•è„šæœ¬å·²åˆ›å»º: test_rag.py")

if __name__ == "__main__":
    # åˆ›å»ºå¹¶æµ‹è¯•çŸ¥è¯†åº“
    print("ğŸš€ åˆå§‹åŒ–RAGç³»ç»Ÿ...")
    
    kb = KnowledgeBase()
    kb.build_index()
    kb.save_knowledge_base()
    
    # æµ‹è¯•æ£€ç´¢åŠŸèƒ½
    test_queries = ["ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ", "Pythonç¼–ç¨‹", "æ·±åº¦å­¦ä¹ "]
    
    for query in test_queries:
        print(f"\\nğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
        results = kb.search(query, top_k=2)
        for result in results:
            print(f"   ğŸ“„ {result['metadata']['title']} (ç›¸ä¼¼åº¦: {result['similarity']:.3f})")
    
    # åˆ›å»ºæµ‹è¯•è„šæœ¬
    create_rag_test_script()
    
    print("\\nâœ… RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
