#!/usr/bin/env python3
"""
å¿«é€Ÿæ¼”ç¤ºè„šæœ¬ - å±•ç¤ºæç¤ºè¯å·¥ç¨‹å’ŒRAGæŠ€æœ¯çš„æ•ˆæœ
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from rag_system import RAGSystem

def quick_demo():
    """å¿«é€Ÿæ¼”ç¤ºä¸‰ç§æ¨¡å¼çš„æ•ˆæœå·®å¼‚"""
    print("ğŸš€ å¤§æ¨¡å‹SFT + æç¤ºè¯å·¥ç¨‹ + RAG æŠ€æœ¯æ¼”ç¤º")
    print("="*70)
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    model_path = "./output/final_model"
    base_model = "distilgpt2"
    
    print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
    
    # åŠ è½½æ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(base_model)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    if os.path.exists(model_path):
        base = AutoModelForCausalLM.from_pretrained(base_model, torch_dtype=torch.float16, device_map="auto")
        model = PeftModel.from_pretrained(base, model_path)
        print("âœ… ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹")
    else:
        model = AutoModelForCausalLM.from_pretrained(base_model, torch_dtype=torch.float16, device_map="auto")
        print("âš ï¸ ä½¿ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œæ¼”ç¤º")
    
    model.eval()
    
    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    print("ğŸ” åˆå§‹åŒ–RAGç³»ç»Ÿ...")
    try:
        from rag_system import KnowledgeBase
        kb = KnowledgeBase()
        kb.build_index()
        print("âœ… RAGç³»ç»Ÿå‡†å¤‡å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸ RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        kb = None
    
    # æ¼”ç¤ºé—®é¢˜
    demo_question = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
    
    print(f"\nğŸ¯ æ¼”ç¤ºé—®é¢˜: {demo_question}")
    print("="*70)
    
    # 1. åŸºç¡€æ¨¡å¼
    print("\nğŸ”¸ æ¨¡å¼1: åŸºç¡€å›ç­”")
    print("-" * 30)
    basic_prompt = f"Human: {demo_question}\nAssistant:"
    inputs = tokenizer(basic_prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=150, temperature=0.7, do_sample=True, pad_token_id=tokenizer.eos_token_id)
    
    basic_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    basic_answer = basic_response[len(basic_prompt):].strip()
    print(f"å›ç­”: {basic_answer}")
    
    # 2. æç¤ºè¯å·¥ç¨‹æ¨¡å¼
    print("\nğŸ”¸ æ¨¡å¼2: æç¤ºè¯å·¥ç¨‹å¢å¼º")
    print("-" * 30)
    pe_prompt = f"""ä½ æ˜¯ä¸€åèµ„æ·±çš„äººå·¥æ™ºèƒ½ä¸“å®¶ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„è¯¦ç»†å›ç­”é—®é¢˜ï¼š

ã€å®šä¹‰ã€‘
ã€ä¸»è¦ç±»å‹ã€‘
ã€åº”ç”¨åœºæ™¯ã€‘

é—®é¢˜: {demo_question}

è¯·æä¾›ä¸“ä¸šã€å‡†ç¡®çš„å›ç­”:"""
    
    inputs = tokenizer(pe_prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.7, do_sample=True, pad_token_id=tokenizer.eos_token_id)
    
    pe_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    pe_answer = pe_response[len(pe_prompt):].strip()
    print(f"å›ç­”: {pe_answer}")
    
    # 3. RAGå¢å¼ºæ¨¡å¼
    print("\nğŸ”¸ æ¨¡å¼3: RAGæ£€ç´¢å¢å¼º")
    print("-" * 30)
    
    if kb:
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        relevant_docs = kb.search(demo_question, top_k=2)
        
        if relevant_docs:
            print(f"ğŸ“š æ£€ç´¢åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£:")
            for doc in relevant_docs:
                print(f"   - {doc['metadata']['title']} (ç›¸ä¼¼åº¦: {doc['similarity']:.3f})")
            
            # æ„å»ºRAGæç¤º
            context = "\n".join([f"å‚è€ƒèµ„æ–™: {doc['content'][:150]}..." for doc in relevant_docs])
            rag_prompt = f"""{context}

åŸºäºä¸Šè¿°å‚è€ƒèµ„æ–™ï¼Œè¯·å›ç­”: {demo_question}

å›ç­”:"""
            
            inputs = tokenizer(rag_prompt, return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.7, do_sample=True, pad_token_id=tokenizer.eos_token_id)
            
            rag_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            rag_answer = rag_response[len(rag_prompt):].strip()
            print(f"å›ç­”: {rag_answer}")
        else:
            print("âŒ æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£")
    else:
        print("âŒ RAGç³»ç»Ÿä¸å¯ç”¨")
    
    # æ€»ç»“
    print("\n" + "="*70)
    print("ğŸ“Š ä¸‰ç§æ¨¡å¼å¯¹æ¯”æ€»ç»“:")
    print("="*70)
    print("ğŸ”¸ åŸºç¡€æ¨¡å¼:")
    print("   - ä¼˜ç‚¹: é€Ÿåº¦å¿«ï¼Œèµ„æºå ç”¨å°‘")
    print("   - ç¼ºç‚¹: å›ç­”å¯èƒ½ä¸å¤Ÿå‡†ç¡®æˆ–è¯¦ç»†")
    print()
    print("ğŸ”¸ æç¤ºè¯å·¥ç¨‹:")
    print("   - ä¼˜ç‚¹: å›ç­”æ›´ç»“æ„åŒ–ï¼Œè´¨é‡æå‡")
    print("   - ç¼ºç‚¹: ä»ä¾èµ–æ¨¡å‹è®­ç»ƒæ—¶çš„çŸ¥è¯†")
    print()
    print("ğŸ”¸ RAGå¢å¼º:")
    print("   - ä¼˜ç‚¹: åŸºäºå®é™…çŸ¥è¯†åº“ï¼Œå›ç­”å‡†ç¡®å¯é ")
    print("   - ç¼ºç‚¹: é€Ÿåº¦ç›¸å¯¹è¾ƒæ…¢ï¼Œéœ€è¦ç»´æŠ¤çŸ¥è¯†åº“")
    print()
    print("ğŸ¯ æ¨èä½¿ç”¨åœºæ™¯:")
    print("   - å¿«é€Ÿé—®ç­”: åŸºç¡€æ¨¡å¼")
    print("   - ç»“æ„åŒ–è¾“å‡º: æç¤ºè¯å·¥ç¨‹")
    print("   - ä¸“ä¸šé¢†åŸŸ: RAGå¢å¼ºæ¨¡å¼")
    print("="*70)

if __name__ == "__main__":
    quick_demo()
