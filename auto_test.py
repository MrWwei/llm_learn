#!/usr/bin/env python3
"""
è‡ªåŠ¨æ‰¹é‡æµ‹è¯•è„šæœ¬ - æ— éœ€äº¤äº’è¾“å…¥
"""

import os
import torch
import json
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from datetime import datetime

os.environ["TOKENIZERS_PARALLELISM"] = "false"

def auto_test():
    """è‡ªåŠ¨æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è‡ªåŠ¨æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹...")
    print("="*60)
    
    model_path = "./output/final_model"
    base_model = "distilgpt2"
    
    # æ£€æŸ¥æ¨¡å‹
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return
    
    try:
        print("ğŸ“¦ åŠ è½½æ¨¡å‹å’Œtokenizer...")
        
        # åŠ è½½æ¨¡å‹
        tokenizer = AutoTokenizer.from_pretrained(base_model)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        base = AutoModelForCausalLM.from_pretrained(
            base_model,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        model = PeftModel.from_pretrained(base, model_path)
        model.eval()
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"   æ€»å‚æ•°: {total_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"   è®¾å¤‡: {next(model.parameters()).device}")
        
        # æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            {
                "category": "è‹±æ–‡AIé—®é¢˜",
                "prompts": [
                    "What is machine learning?",
                    "How does artificial intelligence work?",
                    "Explain neural networks",
                    "What are the benefits of AI?"
                ]
            },
            {
                "category": "ä¸­æ–‡AIé—®é¢˜", 
                "prompts": [
                    "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
                    "æœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆç”¨ï¼Ÿ",
                    "æ·±åº¦å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ",
                    "ç¥ç»ç½‘ç»œå¦‚ä½•å·¥ä½œï¼Ÿ"
                ]
            },
            {
                "category": "æŠ€æœ¯é—®é¢˜",
                "prompts": [
                    "How does a computer work?",
                    "What is programming?",
                    "Explain algorithms",
                    "ä»€ä¹ˆæ˜¯æ•°æ®ç»“æ„ï¼Ÿ"
                ]
            }
        ]
        
        results = []
        test_count = 0
        
        print("\nğŸ§ª å¼€å§‹æ‰¹é‡æµ‹è¯•...")
        print("="*60)
        
        for category_data in test_cases:
            category = category_data["category"]
            prompts = category_data["prompts"]
            
            print(f"\nğŸ“‚ æµ‹è¯•ç±»åˆ«: {category}")
            print("-" * 40)
            
            for i, question in enumerate(prompts, 1):
                test_count += 1
                prompt = f"Human: {question}\nAssistant:"
                
                print(f"\næµ‹è¯• {test_count}: {question}")
                
                # ç”Ÿæˆå›ç­”
                inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
                
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=100,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id,
                        repetition_penalty=1.1
                    )
                
                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                answer = response[len(prompt):].strip()
                
                print(f"å›ç­”: {answer}")
                
                # ä¿å­˜ç»“æœ
                results.append({
                    "category": category,
                    "question": question,
                    "prompt": prompt,
                    "answer": answer,
                    "timestamp": datetime.now().isoformat()
                })
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        output_file = f"test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print("\n" + "="*60)
        print("âœ… æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“Š æ€»å…±æµ‹è¯•: {test_count} ä¸ªé—®é¢˜")
        print(f"ğŸ’¾ ç»“æœä¿å­˜è‡³: {output_file}")
        print("="*60)
        
        # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœæ‘˜è¦
        print("\nğŸ“‹ æµ‹è¯•ç»“æœæ‘˜è¦:")
        for category_data in test_cases:
            category = category_data["category"]
            category_results = [r for r in results if r["category"] == category]
            print(f"  {category}: {len(category_results)} ä¸ªæµ‹è¯•")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    auto_test()
